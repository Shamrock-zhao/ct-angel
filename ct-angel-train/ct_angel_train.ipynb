{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Import dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import time\n",
    "from imageio import imread\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from unet_pp import Xnet\n",
    "from unet_pp.data import *\n",
    "from keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the root path of train data\n",
    "data_path = r'./data'\n",
    "# train image path\n",
    "images_folder = 'train_images'\n",
    "# train mask path\n",
    "labels_folder = 'train_labels'\n",
    "\n",
    "# val image path\n",
    "valid_images_folder = 'val_images'\n",
    "# val mask path\n",
    "valid_labels_folder = 'val_labels'\n",
    "\n",
    "\n",
    "# model save path\n",
    "cache_path = data_path + '/cache'\n",
    "\n",
    "save_model_filename = 'ct_angel'\n",
    "\n",
    "batch_size = 2\n",
    "epochs = 2\n",
    "\n",
    "img_size = 512\n",
    "train_samples = 4   # change according to actual\n",
    "valid_samples = 2   # change according to actual\n",
    "# input_size\n",
    "input_size = (img_size, img_size, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data\n",
    "train_gen_args = dict(rotation_range=0.2,\n",
    "                    width_shift_range=0.05,\n",
    "                    height_shift_range=0.05,\n",
    "                    shear_range=0.05,\n",
    "                    zoom_range=0.05,\n",
    "                    horizontal_flip=True,\n",
    "                    fill_mode='nearest')\n",
    "\n",
    "trainGene = trainGenerator(batch_size, data_path, images_folder, labels_folder, train_gen_args,\n",
    "                        image_color_mode=\"rgb\", mask_color_mode=\"grayscale\",\n",
    "                        target_size=(img_size,img_size))\n",
    "\n",
    "# valid data\n",
    "valid_gen_args = dict(fill_mode='nearest')\n",
    "\n",
    "validGene = trainGenerator(batch_size, data_path, valid_images_folder, valid_labels_folder, valid_gen_args,\n",
    "                        image_color_mode=\"rgb\", mask_color_mode=\"grayscale\",\n",
    "                        target_size=(img_size,img_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model save path\n",
    "if not os.path.exists(cache_path):\n",
    "    os.mkdir(cache_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Administrator.DESKTOP-L8FVK5M\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# create unet_pp model and set params\n",
    "\n",
    "model = Xnet(input_shape=input_size, backbone_name='resnet50', encoder_weights='imagenet', decoder_block_type='transpose')\n",
    "\n",
    "model.compile(optimizer = Adam(lr = 1e-4), loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "now = time.strftime(\"%y-%m-%d_%H_%M\", time.localtime(time.time()))\n",
    "\n",
    "model_name = save_model_filename + \"_\" + now + '_{epoch:02d}-{val_acc:.3f}.hdf5'\n",
    "abs_model_name = os.path.join(cache_path, model_name)\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(abs_model_name, monitor='val_loss', verbose=2, save_best_only=True)\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10)\n",
    "callbacks = [early_stop, model_checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Administrator.DESKTOP-L8FVK5M\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/2\n",
      "Found 2 images belonging to 1 classes.\n",
      "Found 4 images belonging to 1 classes.\n",
      "Found 4 images belonging to 1 classes.\n",
      "Found 2 images belonging to 1 classes.\n",
      "2/2 [==============================] - 21s 11s/step - loss: 0.6574 - acc: 0.6127 - val_loss: 0.6311 - val_acc: 0.6486\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.63108, saving model to ./data/cache\\ct_angel_20-03-14_10_46_01-0.649.hdf5\n",
      "Epoch 2/2\n",
      "2/2 [==============================] - 1s 405ms/step - loss: 0.5981 - acc: 0.7006 - val_loss: 0.5508 - val_acc: 0.7393\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.63108 to 0.55077, saving model to ./data/cache\\ct_angel_20-03-14_10_46_02-0.739.hdf5\n"
     ]
    }
   ],
   "source": [
    "# begin train\n",
    "history = model.fit_generator(trainGene, steps_per_epoch=train_samples // batch_size, epochs=epochs, \n",
    "                    validation_data=validGene, validation_steps=valid_samples // batch_size, \n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load:  ./data/cache\\ct_angel_20-03-14_10_38_02-0.770.hdf5\n"
     ]
    }
   ],
   "source": [
    "# load model weights\n",
    "\n",
    "model_file_name = 'ct_angel_20-03-14_10_38_02-0.770.hdf5'\n",
    "model_path = os.path.join(cache_path, model_file_name)\n",
    "\n",
    "model = Xnet(input_shape=input_size, backbone_name='resnet50', encoder_weights='imagenet', decoder_block_type='transpose')\n",
    "\n",
    "print('load: ', model_path)\n",
    "model.load_weights(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict and save image\n",
    "test_img_path = data_path + r'\\test_images'\n",
    "test_label_path = data_path + r'\\test_labels'\n",
    "result_path = data_path + r'\\prd_result'\n",
    "if not os.path.exists(result_path):\n",
    "    os.mkdir(result_path)\n",
    "    \n",
    "test_files = os.listdir(test_img_path)\n",
    "predict_area_num = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def takeLength(elem):\n",
    "    return len(elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 4s 4s/step\n",
      "1/1 [==============================] - 0s 65ms/step\n"
     ]
    }
   ],
   "source": [
    "for file in test_files:\n",
    "    fpath,fname = os.path.split(file)\n",
    "\n",
    "    out_fname = os.path.join(result_path, fname)\n",
    "\n",
    "    \n",
    "    src_img = imread(os.path.join(test_img_path, file))\n",
    "    img = src_img\n",
    "    \n",
    "    img = trans.resize(img,(img_size,img_size))\n",
    "    \n",
    "    # img from (512, 512, 3) trans to (1, 512, 512, 3)\n",
    "    img = np.reshape(img,(1,)+img.shape)\n",
    "\n",
    "\n",
    "    # predict\n",
    "    results = model.predict(np.array(img),verbose=1)\n",
    "\n",
    "    gray_img = results[0][:,:,0]\n",
    "\n",
    "    # to gray\n",
    "    bw_heatmap = np.uint8(255 * gray_img)\n",
    "    bw_heatmap[bw_heatmap <= 127] = 0\n",
    "    bw_heatmap[bw_heatmap > 127] = 255\n",
    "    _, ai_ctrs, _ = cv2.findContours(bw_heatmap.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    ai_ctrs.sort(key=takeLength, reverse=True)\n",
    "\n",
    "    # choose the bigger \n",
    "    ai_ctrs = ai_ctrs[:predict_area_num]\n",
    "\n",
    "    # draw box\n",
    "    for c in ai_ctrs:        \n",
    "        # find bounding box coordinates\n",
    "        x,y,w,h = cv2.boundingRect(c)\n",
    "        cv2.rectangle(src_img, (x,y), (x+w, y+h), (255, 0, 0), 1)\n",
    "        \n",
    "\n",
    "    # save result image\n",
    "    io.imsave(out_fname, src_img)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
